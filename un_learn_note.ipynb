{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexandre\\miniconda3\\envs\\llama_ssl\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType\n",
    ")\n",
    "from transformers import BitsAndBytesConfig\n",
    "from typing import List, Optional, Union, Any\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_type_check(value: Any, expected_type: type, name: str):\n",
    "    \"\"\"\n",
    "    Perform detailed type checking with informative error messages\n",
    "    \"\"\"\n",
    "    if not isinstance(value, expected_type):\n",
    "        raise TypeError(f\"Expected {name} to be {expected_type}, but got {type(value)}: {value}\")\n",
    "\n",
    "class LlamaSemiSupervised:\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_name: str = \"meta-llama/Llama-3.2-1B\",\n",
    "        num_labels: int = 2,\n",
    "        batch_size: int = 16,\n",
    "        num_epochs: int = 3,\n",
    "        lora_r=8,          # Reduced rank\n",
    "        lora_alpha=16,     # Reduced alpha\n",
    "        lora_dropout=0.1,\n",
    "        learning_rate: float = 2e-4,\n",
    "        max_length: int = 512\n",
    "    ):\n",
    "        \n",
    "        print(f\"Available GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9} GB\")\n",
    "        \n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,  # Switch to 8-bit quantization\n",
    "            llm_int8_threshold=6.0,\n",
    "            llm_int8_has_fp16_weight=False\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Validate inputs\n",
    "        debug_type_check(model_name, str, \"model_name\")\n",
    "        debug_type_check(num_labels, int, \"num_labels\")\n",
    "        debug_type_check(batch_size, int, \"batch_size\")\n",
    "        debug_type_check(num_epochs, int, \"num_epochs\")\n",
    "        debug_type_check(learning_rate, (int, float), \"learning_rate\")\n",
    "        debug_type_check(max_length, int, \"max_length\")\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.num_labels = num_labels\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Debug print for initial configuration\n",
    "        print(\"Initializing model with:\")\n",
    "        print(f\"Model: {model_name}\")\n",
    "        print(f\"Number of Labels: {num_labels}\")\n",
    "        \n",
    "        # Initialize tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            padding_side='right',\n",
    "            truncation_side='right',\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Handle pad token\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map='auto',\n",
    "            torch_dtype=torch.float16,\n",
    "            max_memory={0: '10GB'},  # Explicitly limit GPU memory\n",
    "            trust_remote_code=True,\n",
    "            pad_token_id=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Prepare for k-bit training\n",
    "        self.model = prepare_model_for_kbit_training(self.model)\n",
    "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # LoRA Configuration\n",
    "        lora_config = LoraConfig(\n",
    "            r=lora_r,\n",
    "            lora_alpha=lora_alpha,\n",
    "            target_modules=[\"q_proj\", \"v_proj\"],\n",
    "            lora_dropout=lora_dropout,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.SEQ_CLS\n",
    "        )\n",
    "        \n",
    "        # Get PEFT model\n",
    "        self.model = get_peft_model(self.model, lora_config)\n",
    "        \n",
    "        self.model = self.model.to(device)\n",
    "\n",
    "    def _validate_inputs(\n",
    "        self, \n",
    "        texts: List[str], \n",
    "        labels: Optional[List[int]] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Comprehensive input validation\n",
    "        \"\"\"\n",
    "        # Validate texts\n",
    "        debug_type_check(texts, list, \"texts\")\n",
    "        if not all(isinstance(text, str) for text in texts):\n",
    "            raise TypeError(\"All texts must be strings\")\n",
    "        \n",
    "        # Validate labels if provided\n",
    "        if labels is not None:\n",
    "            debug_type_check(labels, list, \"labels\")\n",
    "            if not all(isinstance(label, (int, np.integer)) for label in labels):\n",
    "                raise TypeError(\"All labels must be integers\")\n",
    "            \n",
    "            # Check label range\n",
    "            unique_labels = set(labels)\n",
    "            if len(unique_labels) > self.num_labels:\n",
    "                raise ValueError(f\"More unique labels ({len(unique_labels)}) than specified num_labels ({self.num_labels})\")\n",
    "            \n",
    "            if len(texts) != len(labels):\n",
    "                raise ValueError(f\"Mismatch in texts ({len(texts)}) and labels ({len(labels)}) lengths\")\n",
    "    \n",
    "    def prepare_data(\n",
    "        self, \n",
    "        texts: List[str], \n",
    "        labels: Optional[List[int]] = None\n",
    "    ) -> Dataset:\n",
    "        \"\"\"\n",
    "        Prepare data with extensive validation\n",
    "        \"\"\"\n",
    "        # Validate inputs\n",
    "        self._validate_inputs(texts, labels)\n",
    "        \n",
    "        # Convert labels to list of integers (numpy or python int)\n",
    "        if labels is not None:\n",
    "            labels = [int(label) for label in labels]\n",
    "        \n",
    "        # Tokenize texts\n",
    "        tokenized = self.tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Create dataset\n",
    "        if labels is not None:\n",
    "            print(f\"Preparing dataset with {len(texts)} texts and {len(labels)} labels\")\n",
    "            return Dataset.from_dict({\n",
    "                'input_ids': tokenized['input_ids'],\n",
    "                'attention_mask': tokenized['attention_mask'],\n",
    "                'labels': labels\n",
    "            })\n",
    "        return Dataset.from_dict({\n",
    "            'input_ids': tokenized['input_ids'],\n",
    "            'attention_mask': tokenized['attention_mask']\n",
    "        })\n",
    "    \n",
    "    def train(\n",
    "        self, \n",
    "        train_texts: List[str], \n",
    "        train_labels: List[int],\n",
    "        val_texts: Optional[List[str]] = None, \n",
    "        val_labels: Optional[List[int]] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Training with comprehensive input validation\n",
    "        \"\"\"\n",
    "        # Validate inputs\n",
    "        self._validate_inputs(train_texts, train_labels)\n",
    "        if val_texts is not None and val_labels is not None:\n",
    "            self._validate_inputs(val_texts, val_labels)\n",
    "        \n",
    "        # Prepare training data\n",
    "        train_dataset = self.prepare_data(train_texts, train_labels)\n",
    "        \n",
    "        # Prepare validation data\n",
    "        if val_texts is not None and val_labels is not None:\n",
    "            val_dataset = self.prepare_data(val_texts, val_labels)\n",
    "        else:\n",
    "            # Manual split for Hugging Face Dataset\n",
    "            total_size = len(train_dataset)\n",
    "            val_size = int(total_size * 0.1)\n",
    "            train_size = total_size - val_size\n",
    "            \n",
    "            val_dataset = train_dataset.select(range(val_size))\n",
    "            train_dataset = train_dataset.select(range(val_size, total_size))\n",
    "        \n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"./results\",\n",
    "            num_train_epochs=self.num_epochs,\n",
    "            per_device_train_batch_size=self.batch_size,\n",
    "            per_device_eval_batch_size=self.batch_size,\n",
    "            gradient_accumulation_steps=4,\n",
    "            warmup_ratio=0.1,\n",
    "            learning_rate=self.learning_rate,\n",
    "            logging_dir=\"./logs\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            save_total_limit=2,\n",
    "            load_best_model_at_end=True,\n",
    "            fp16=True,\n",
    "            optim=\"adamw_torch\",\n",
    "            weight_decay=0.01\n",
    "        )\n",
    "        \n",
    "        # Initialize trainer\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            data_collator=DataCollatorWithPadding(self.tokenizer)\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        trainer.train()\n",
    "    \n",
    "    def predict(self, texts: List[str]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generate predictions with input validation\n",
    "        \"\"\"\n",
    "        # Validate input\n",
    "        self._validate_inputs(texts)\n",
    "        \n",
    "        # Prepare dataset\n",
    "        dataset = self.prepare_data(texts)\n",
    "        \n",
    "        # Create trainer for prediction\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer\n",
    "        )\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = trainer.predict(dataset)\n",
    "        return torch.nn.functional.softmax(\n",
    "            torch.tensor(predictions.predictions), \n",
    "            dim=1\n",
    "        )\n",
    "    \n",
    "    def semi_supervised_learning(\n",
    "        self,\n",
    "        labelled_texts: List[str],\n",
    "        labelled_labels: List[int],\n",
    "        unlabelled_texts: List[str],\n",
    "        confidence_threshold: float = 0.9\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Semi-supervised learning pipeline with comprehensive validation\n",
    "        \"\"\"\n",
    "        # Validate inputs\n",
    "        self._validate_inputs(labelled_texts, labelled_labels)\n",
    "        self._validate_inputs(unlabelled_texts)\n",
    "        \n",
    "        # Initial training\n",
    "        self.train(labelled_texts, labelled_labels)\n",
    "        \n",
    "        # Predict on unlabelled data\n",
    "        probabilities = self.predict(unlabelled_texts)\n",
    "        predictions = torch.argmax(probabilities, dim=1)\n",
    "        max_probs = torch.max(probabilities, dim=1)[0]\n",
    "        \n",
    "        # Filter high-confidence predictions\n",
    "        confident_idx = max_probs >= confidence_threshold\n",
    "        new_labels = predictions[confident_idx]\n",
    "        new_texts = [\n",
    "            unlabelled_texts[i] \n",
    "            for i in range(len(unlabelled_texts)) \n",
    "            if confident_idx[i]\n",
    "        ]\n",
    "        \n",
    "        # Combine datasets\n",
    "        all_texts = labelled_texts + new_texts\n",
    "        all_labels = labelled_labels + new_labels.tolist()\n",
    "        \n",
    "        # Retrain\n",
    "        self.train(all_texts, all_labels)\n",
    "        \n",
    "        # Final predictions\n",
    "        final_probabilities = self.predict(unlabelled_texts)\n",
    "        return torch.argmax(final_probabilities, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>Row Number</th>\n",
       "      <th>Tweet Treated</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fffeb77ff91c618cc5482e982240f1af9f09175cddf324...</td>\n",
       "      <td>19999</td>\n",
       "      <td>russia would save a ton of money if they'd pul...</td>\n",
       "      <td>Non Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ffe7a960fccf628755ee70ab15e4fab5b45f0f436a2064...</td>\n",
       "      <td>19998</td>\n",
       "      <td>i hate grocery shopping. spent $112. damn you ...</td>\n",
       "      <td>Non Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ffdc308f7c1ceed12d8347ab9150551a9fe155023d624a...</td>\n",
       "      <td>19997</td>\n",
       "      <td>did you miss his blood and soil arguments f...</td>\n",
       "      <td>Non Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ffdbe9613a9ef0c6c484486e03422ab0bac73f62922005...</td>\n",
       "      <td>19996</td>\n",
       "      <td>on imperialism, too complex for twitter, but ...</td>\n",
       "      <td>Non Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ffd75a28b8bf37f681e5d57d1dd1309df03aa3859a3d28...</td>\n",
       "      <td>19995</td>\n",
       "      <td>i'm still wondering why we don't #stop doing #...</td>\n",
       "      <td>Non Hate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              row_id  Row Number  \\\n",
       "0  fffeb77ff91c618cc5482e982240f1af9f09175cddf324...       19999   \n",
       "1  ffe7a960fccf628755ee70ab15e4fab5b45f0f436a2064...       19998   \n",
       "2  ffdc308f7c1ceed12d8347ab9150551a9fe155023d624a...       19997   \n",
       "3  ffdbe9613a9ef0c6c484486e03422ab0bac73f62922005...       19996   \n",
       "4  ffd75a28b8bf37f681e5d57d1dd1309df03aa3859a3d28...       19995   \n",
       "\n",
       "                                       Tweet Treated     Label  \n",
       "0  russia would save a ton of money if they'd pul...  Non Hate  \n",
       "1  i hate grocery shopping. spent $112. damn you ...  Non Hate  \n",
       "2     did you miss his blood and soil arguments f...  Non Hate  \n",
       "3   on imperialism, too complex for twitter, but ...  Non Hate  \n",
       "4  i'm still wondering why we don't #stop doing #...  Non Hate  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs_df = pd.read_csv(\"hs_df.csv\")\n",
    "hs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>Row Number</th>\n",
       "      <th>Tweet Treated</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>fd03af0faea32d6ba18a883aa41d496a0f77a95f0ddfd3...</td>\n",
       "      <td>19768</td>\n",
       "      <td>zelensky and putin both need this lawnmower.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>fc7449ecd89f35f4faa1549d7e99b49b89419f8a690f28...</td>\n",
       "      <td>19719</td>\n",
       "      <td>zelensky's talking trash. how's that for a p...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>fa08dfe829005b4fe02699c489a5273bc66afa827fb73e...</td>\n",
       "      <td>19536</td>\n",
       "      <td>yeah, zelensky had good intentions. but th...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>f353c03c58dcb208e9e952a00c216558fb32465708997d...</td>\n",
       "      <td>19000</td>\n",
       "      <td>again i am pretty sure china never called its...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>f352d8b1a9d1e5ea36d768fb4a8342784c746c5d393e74...</td>\n",
       "      <td>18999</td>\n",
       "      <td>... which sickened &amp;gt;3000 un troops during t...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 row_id  Row Number  \\\n",
       "231   fd03af0faea32d6ba18a883aa41d496a0f77a95f0ddfd3...       19768   \n",
       "280   fc7449ecd89f35f4faa1549d7e99b49b89419f8a690f28...       19719   \n",
       "463   fa08dfe829005b4fe02699c489a5273bc66afa827fb73e...       19536   \n",
       "999   f353c03c58dcb208e9e952a00c216558fb32465708997d...       19000   \n",
       "1000  f352d8b1a9d1e5ea36d768fb4a8342784c746c5d393e74...       18999   \n",
       "\n",
       "                                          Tweet Treated Label  \n",
       "231       zelensky and putin both need this lawnmower.    NaN  \n",
       "280     zelensky's talking trash. how's that for a p...   NaN  \n",
       "463       yeah, zelensky had good intentions. but th...   NaN  \n",
       "999    again i am pretty sure china never called its...   NaN  \n",
       "1000  ... which sickened &gt;3000 un troops during t...   NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs_df_labelled = hs_df[hs_df[\"Label\"].notnull()]\n",
    "hs_df_unlabelled = hs_df[hs_df[\"Label\"].isnull()]\n",
    "\n",
    "hs_df_unlabelled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexandre\\AppData\\Local\\Temp\\ipykernel_9148\\3592870323.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  hs_df_labelled[\"Label_bool\"] = hs_df_labelled[\"Label\"].apply(lambda x: 1 if x == \"Hate\" else 0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>Row Number</th>\n",
       "      <th>Tweet Treated</th>\n",
       "      <th>Label</th>\n",
       "      <th>Label_bool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fffeb77ff91c618cc5482e982240f1af9f09175cddf324...</td>\n",
       "      <td>19999</td>\n",
       "      <td>russia would save a ton of money if they'd pul...</td>\n",
       "      <td>Non Hate</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ffe7a960fccf628755ee70ab15e4fab5b45f0f436a2064...</td>\n",
       "      <td>19998</td>\n",
       "      <td>i hate grocery shopping. spent $112. damn you ...</td>\n",
       "      <td>Non Hate</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ffdc308f7c1ceed12d8347ab9150551a9fe155023d624a...</td>\n",
       "      <td>19997</td>\n",
       "      <td>did you miss his blood and soil arguments f...</td>\n",
       "      <td>Non Hate</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ffdbe9613a9ef0c6c484486e03422ab0bac73f62922005...</td>\n",
       "      <td>19996</td>\n",
       "      <td>on imperialism, too complex for twitter, but ...</td>\n",
       "      <td>Non Hate</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ffd75a28b8bf37f681e5d57d1dd1309df03aa3859a3d28...</td>\n",
       "      <td>19995</td>\n",
       "      <td>i'm still wondering why we don't #stop doing #...</td>\n",
       "      <td>Non Hate</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>f361154d56c09c343ebc7d0f967c492eca88a0bbcd03fe...</td>\n",
       "      <td>19005</td>\n",
       "      <td>putin gave israeli pm bennett his word he woul...</td>\n",
       "      <td>Non Hate</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>f35e48848e79e00d6bbbf10ca13c5e2287fcf07d6ecaf6...</td>\n",
       "      <td>19004</td>\n",
       "      <td>does anyone else ever feel like joy-crying whe...</td>\n",
       "      <td>Non Hate</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>f35abbe1259d13efb4031780f3226eb5ea20a385d225b2...</td>\n",
       "      <td>19003</td>\n",
       "      <td>nuremberg aint gonna actually happen until the...</td>\n",
       "      <td>Non Hate</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>f35aace2208a8384202e020cfdb221889de2bcb87dea7d...</td>\n",
       "      <td>19002</td>\n",
       "      <td>their gov is also more corrupt than russi...</td>\n",
       "      <td>Non Hate</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>f3559624799a6fdb0e0748ef6bee9dc32101f4722456e5...</td>\n",
       "      <td>19001</td>\n",
       "      <td>it's his \"i'm a man of the people\" look. kind...</td>\n",
       "      <td>Non Hate</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>996 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                row_id  Row Number  \\\n",
       "0    fffeb77ff91c618cc5482e982240f1af9f09175cddf324...       19999   \n",
       "1    ffe7a960fccf628755ee70ab15e4fab5b45f0f436a2064...       19998   \n",
       "2    ffdc308f7c1ceed12d8347ab9150551a9fe155023d624a...       19997   \n",
       "3    ffdbe9613a9ef0c6c484486e03422ab0bac73f62922005...       19996   \n",
       "4    ffd75a28b8bf37f681e5d57d1dd1309df03aa3859a3d28...       19995   \n",
       "..                                                 ...         ...   \n",
       "994  f361154d56c09c343ebc7d0f967c492eca88a0bbcd03fe...       19005   \n",
       "995  f35e48848e79e00d6bbbf10ca13c5e2287fcf07d6ecaf6...       19004   \n",
       "996  f35abbe1259d13efb4031780f3226eb5ea20a385d225b2...       19003   \n",
       "997  f35aace2208a8384202e020cfdb221889de2bcb87dea7d...       19002   \n",
       "998  f3559624799a6fdb0e0748ef6bee9dc32101f4722456e5...       19001   \n",
       "\n",
       "                                         Tweet Treated     Label  Label_bool  \n",
       "0    russia would save a ton of money if they'd pul...  Non Hate           0  \n",
       "1    i hate grocery shopping. spent $112. damn you ...  Non Hate           0  \n",
       "2       did you miss his blood and soil arguments f...  Non Hate           0  \n",
       "3     on imperialism, too complex for twitter, but ...  Non Hate           0  \n",
       "4    i'm still wondering why we don't #stop doing #...  Non Hate           0  \n",
       "..                                                 ...       ...         ...  \n",
       "994  putin gave israeli pm bennett his word he woul...  Non Hate           0  \n",
       "995  does anyone else ever feel like joy-crying whe...  Non Hate           0  \n",
       "996  nuremberg aint gonna actually happen until the...  Non Hate           0  \n",
       "997       their gov is also more corrupt than russi...  Non Hate           0  \n",
       "998   it's his \"i'm a man of the people\" look. kind...  Non Hate           0  \n",
       "\n",
       "[996 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs_df_labelled[\"Label_bool\"] = hs_df_labelled[\"Label\"].apply(lambda x: 1 if x == \"Hate\" else 0)\n",
    "hs_df_labelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_texts = hs_df_labelled[\"Tweet Treated\"].tolist()\n",
    "labelled_labels = hs_df_labelled[\"Label_bool\"].tolist()\n",
    "\n",
    "unlabelled_texts = hs_df_unlabelled[\"Tweet Treated\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPU Memory: 12.878086144 GB\n",
      "Initializing model with:\n",
      "Model: meta-llama/Llama-3.2-1B\n",
      "Number of Labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Alexandre\\miniconda3\\envs\\llama_ssl\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset with 996 texts and 996 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/42 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "c:\\Users\\Alexandre\\miniconda3\\envs\\llama_ssl\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\Alexandre\\miniconda3\\envs\\llama_ssl\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "                                               \n",
      " 33%|███▎      | 14/42 [01:05<01:58,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.573652982711792, 'eval_runtime': 2.0488, 'eval_samples_per_second': 48.321, 'eval_steps_per_second': 3.417, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexandre\\miniconda3\\envs\\llama_ssl\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\Alexandre\\miniconda3\\envs\\llama_ssl\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "                                               \n",
      " 67%|██████▋   | 28/42 [02:11<01:02,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4896319508552551, 'eval_runtime': 2.4913, 'eval_samples_per_second': 39.738, 'eval_steps_per_second': 2.81, 'epoch': 1.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexandre\\miniconda3\\envs\\llama_ssl\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\Alexandre\\miniconda3\\envs\\llama_ssl\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "100%|██████████| 42/42 [03:13<00:00,  4.58s/it]c:\\Users\\Alexandre\\miniconda3\\envs\\llama_ssl\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "                                               \n",
      "100%|██████████| 42/42 [03:16<00:00,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5027834177017212, 'eval_runtime': 2.5359, 'eval_samples_per_second': 39.04, 'eval_steps_per_second': 2.76, 'epoch': 2.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [03:16<00:00,  4.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 196.5781, 'train_samples_per_second': 13.689, 'train_steps_per_second': 0.214, 'train_loss': 0.506005605061849, 'epoch': 2.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexandre\\AppData\\Local\\Temp\\ipykernel_9148\\2170216504.py:236: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "c:\\Users\\Alexandre\\miniconda3\\envs\\llama_ssl\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "100%|██████████| 2376/2376 [20:10<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset with 8435 texts and 8435 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexandre\\miniconda3\\envs\\llama_ssl\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "  0%|          | 0/354 [00:00<?, ?it/s]c:\\Users\\Alexandre\\miniconda3\\envs\\llama_ssl\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "                                                  \n",
      " 33%|███▎      | 118/354 [22:21<43:10, 10.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7629305720329285, 'eval_runtime': 39.7299, 'eval_samples_per_second': 21.218, 'eval_steps_per_second': 1.334, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexandre\\miniconda3\\envs\\llama_ssl\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\Alexandre\\miniconda3\\envs\\llama_ssl\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "                                                   \n",
      " 67%|██████▋   | 237/354 [44:39<21:12, 10.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7410508990287781, 'eval_runtime': 39.8264, 'eval_samples_per_second': 21.167, 'eval_steps_per_second': 1.331, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexandre\\miniconda3\\envs\\llama_ssl\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\Alexandre\\miniconda3\\envs\\llama_ssl\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "100%|██████████| 354/354 [1:05:51<00:00, 10.91s/it]c:\\Users\\Alexandre\\miniconda3\\envs\\llama_ssl\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "                                                   \n",
      "100%|██████████| 354/354 [1:06:32<00:00, 10.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8751929998397827, 'eval_runtime': 39.7435, 'eval_samples_per_second': 21.211, 'eval_steps_per_second': 1.334, 'epoch': 2.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 354/354 [1:06:32<00:00, 11.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 3992.5218, 'train_samples_per_second': 5.705, 'train_steps_per_second': 0.089, 'train_loss': 0.008927983079252943, 'epoch': 2.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexandre\\AppData\\Local\\Temp\\ipykernel_9148\\2170216504.py:236: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "c:\\Users\\Alexandre\\miniconda3\\envs\\llama_ssl\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "100%|██████████| 2376/2376 [20:39<00:00,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: tensor([0, 0, 0,  ..., 0, 0, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize and run\n",
    "ssl_model = LlamaSemiSupervised(num_labels=2)\n",
    "predictions = ssl_model.semi_supervised_learning(\n",
    "    labelled_texts,\n",
    "    labelled_labels,\n",
    "    unlabelled_texts\n",
    ")\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 16459, 1: 2545}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def count_predictions(predictions):\n",
    "    unique_elements, counts = torch.unique(predictions, return_counts=True)\n",
    "    return dict(zip(unique_elements.tolist(), counts.tolist()))\n",
    "\n",
    "# Example usage:\n",
    "#predictions = torch.tensor([1, 2, 2, 3, 3, 3, 4, 4, 4, 4])\n",
    "print(count_predictions(predictions))\n",
    "# Output: {1: 1, 2: 2, 3: 3, 4: 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet unlabeled</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zelensky and putin both need this lawnmower.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>zelensky's talking trash. how's that for a p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yeah, zelensky had good intentions. but th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>again i am pretty sure china never called its...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>... which sickened &amp;gt;3000 un troops during t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18999</th>\n",
       "      <td>let ukraine take care of itself.  this so-...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19000</th>\n",
       "      <td>your daily reminder that 141 un countries vo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19001</th>\n",
       "      <td>obvious to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19002</th>\n",
       "      <td>who is this f*cking idiot? biden isn't a neoco...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19003</th>\n",
       "      <td>zelensky, you have stolen enough us tax dolla...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19004 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         tweet unlabeled  pred\n",
       "0          zelensky and putin both need this lawnmower.      0\n",
       "1        zelensky's talking trash. how's that for a p...     0\n",
       "2          yeah, zelensky had good intentions. but th...     0\n",
       "3       again i am pretty sure china never called its...     0\n",
       "4      ... which sickened &gt;3000 un troops during t...     0\n",
       "...                                                  ...   ...\n",
       "18999      let ukraine take care of itself.  this so-...     0\n",
       "19000    your daily reminder that 141 un countries vo...     0\n",
       "19001                                      obvious to...     0\n",
       "19002  who is this f*cking idiot? biden isn't a neoco...     0\n",
       "19003   zelensky, you have stolen enough us tax dolla...     1\n",
       "\n",
       "[19004 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_list = predictions.tolist()\n",
    "\n",
    "unlabelled_pred = pd.DataFrame({'tweet unlabeled': unlabelled_texts, 'pred': predictions_list})\n",
    "unlabelled_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabelled_pred.loc[unlabelled_pred['pred'] == 1].to_csv('unlabelled_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 790, 1: 206})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_predictions(predictions):\n",
    "    return Counter(predictions)\n",
    "\n",
    "# Example usage:\n",
    "print(count_predictions(labelled_labels))\n",
    "# Output: Counter({4: 4, 3: 3, 2: 2, 1: 1})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_ssl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
