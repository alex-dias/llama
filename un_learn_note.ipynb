{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexandre\\miniconda3\\envs\\llama_ssl\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1' \n",
    "#os.environ['CUDA_HOME'] = 'C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v11.8'\n",
    "\n",
    "import pandas as pd\n",
    "#import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType\n",
    ")\n",
    "from transformers import BitsAndBytesConfig\n",
    "from typing import List, Optional, Union, Any\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_type_check(value: Any, expected_type: type, name: str):\n",
    "    \"\"\"\n",
    "    Perform detailed type checking with informative error messages\n",
    "    \"\"\"\n",
    "    if not isinstance(value, expected_type):\n",
    "        raise TypeError(f\"Expected {name} to be {expected_type}, but got {type(value)}: {value}\")\n",
    "\n",
    "class LlamaSemiSupervised:\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_name: str = \"meta-llama/Llama-3.2-1B\",\n",
    "        num_labels: int = 2,\n",
    "        batch_size: int = 16,\n",
    "        num_epochs: int = 3,\n",
    "        lora_r=8,          # Reduced rank\n",
    "        lora_alpha=16,     # Reduced alpha\n",
    "        lora_dropout=0.1,\n",
    "        learning_rate: float = 2e-4,\n",
    "        max_length: int = 512\n",
    "    ):\n",
    "        \n",
    "        print(f\"Available GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9} GB\")\n",
    "        \n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,  # Switch to 8-bit quantization\n",
    "            llm_int8_threshold=6.0,\n",
    "            llm_int8_has_fp16_weight=False\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Validate inputs\n",
    "        debug_type_check(model_name, str, \"model_name\")\n",
    "        debug_type_check(num_labels, int, \"num_labels\")\n",
    "        debug_type_check(batch_size, int, \"batch_size\")\n",
    "        debug_type_check(num_epochs, int, \"num_epochs\")\n",
    "        debug_type_check(learning_rate, (int, float), \"learning_rate\")\n",
    "        debug_type_check(max_length, int, \"max_length\")\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.num_labels = num_labels\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Debug print for initial configuration\n",
    "        print(\"Initializing model with:\")\n",
    "        print(f\"Model: {model_name}\")\n",
    "        print(f\"Number of Labels: {num_labels}\")\n",
    "        \n",
    "        # Initialize tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            padding_side='right',\n",
    "            truncation_side='right',\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Handle pad token\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map='auto',\n",
    "            torch_dtype=torch.float16,\n",
    "            max_memory={0: '10GB'},  # Explicitly limit GPU memory\n",
    "            trust_remote_code=True,\n",
    "            pad_token_id=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Prepare for k-bit training\n",
    "        self.model = prepare_model_for_kbit_training(self.model)\n",
    "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # LoRA Configuration\n",
    "        lora_config = LoraConfig(\n",
    "            r=lora_r,\n",
    "            lora_alpha=lora_alpha,\n",
    "            target_modules=[\"q_proj\", \"v_proj\"],\n",
    "            lora_dropout=lora_dropout,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.SEQ_CLS\n",
    "        )\n",
    "        \n",
    "        # Get PEFT model\n",
    "        self.model = get_peft_model(self.model, lora_config)\n",
    "        \n",
    "        self.model = self.model.to(device)\n",
    "\n",
    "    def _validate_inputs(\n",
    "        self, \n",
    "        texts: List[str], \n",
    "        labels: Optional[List[int]] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Comprehensive input validation\n",
    "        \"\"\"\n",
    "        # Validate texts\n",
    "        debug_type_check(texts, list, \"texts\")\n",
    "        if not all(isinstance(text, str) for text in texts):\n",
    "            raise TypeError(\"All texts must be strings\")\n",
    "        \n",
    "        # Validate labels if provided\n",
    "        if labels is not None:\n",
    "            debug_type_check(labels, list, \"labels\")\n",
    "            if not all(isinstance(label, (int, np.integer)) for label in labels):\n",
    "                raise TypeError(\"All labels must be integers\")\n",
    "            \n",
    "            # Check label range\n",
    "            unique_labels = set(labels)\n",
    "            if len(unique_labels) > self.num_labels:\n",
    "                raise ValueError(f\"More unique labels ({len(unique_labels)}) than specified num_labels ({self.num_labels})\")\n",
    "            \n",
    "            if len(texts) != len(labels):\n",
    "                raise ValueError(f\"Mismatch in texts ({len(texts)}) and labels ({len(labels)}) lengths\")\n",
    "    \n",
    "    def prepare_data(\n",
    "        self, \n",
    "        texts: List[str], \n",
    "        labels: Optional[List[int]] = None\n",
    "    ) -> Dataset:\n",
    "        \"\"\"\n",
    "        Prepare data with extensive validation\n",
    "        \"\"\"\n",
    "        # Validate inputs\n",
    "        self._validate_inputs(texts, labels)\n",
    "        \n",
    "        # Convert labels to list of integers (numpy or python int)\n",
    "        if labels is not None:\n",
    "            labels = [int(label) for label in labels]\n",
    "        \n",
    "        # Tokenize texts\n",
    "        tokenized = self.tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Create dataset\n",
    "        if labels is not None:\n",
    "            print(f\"Preparing dataset with {len(texts)} texts and {len(labels)} labels\")\n",
    "            return Dataset.from_dict({\n",
    "                'input_ids': tokenized['input_ids'],\n",
    "                'attention_mask': tokenized['attention_mask'],\n",
    "                'labels': labels\n",
    "            })\n",
    "        return Dataset.from_dict({\n",
    "            'input_ids': tokenized['input_ids'],\n",
    "            'attention_mask': tokenized['attention_mask']\n",
    "        })\n",
    "    \n",
    "    def train(\n",
    "        self, \n",
    "        train_texts: List[str], \n",
    "        train_labels: List[int],\n",
    "        val_texts: Optional[List[str]] = None, \n",
    "        val_labels: Optional[List[int]] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Training with comprehensive input validation\n",
    "        \"\"\"\n",
    "        # Validate inputs\n",
    "        self._validate_inputs(train_texts, train_labels)\n",
    "        if val_texts is not None and val_labels is not None:\n",
    "            self._validate_inputs(val_texts, val_labels)\n",
    "        \n",
    "        # Prepare training data\n",
    "        train_dataset = self.prepare_data(train_texts, train_labels)\n",
    "        \n",
    "        # Prepare validation data\n",
    "        if val_texts is not None and val_labels is not None:\n",
    "            val_dataset = self.prepare_data(val_texts, val_labels)\n",
    "        else:\n",
    "            # Manual split for Hugging Face Dataset\n",
    "            total_size = len(train_dataset)\n",
    "            val_size = int(total_size * 0.1)\n",
    "            train_size = total_size - val_size\n",
    "            \n",
    "            val_dataset = train_dataset.select(range(val_size))\n",
    "            train_dataset = train_dataset.select(range(val_size, total_size))\n",
    "        \n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"./results\",\n",
    "            num_train_epochs=self.num_epochs,\n",
    "            per_device_train_batch_size=self.batch_size,\n",
    "            per_device_eval_batch_size=self.batch_size,\n",
    "            gradient_accumulation_steps=4,\n",
    "            warmup_ratio=0.1,\n",
    "            learning_rate=self.learning_rate,\n",
    "            logging_dir=\"./logs\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            save_total_limit=2,\n",
    "            load_best_model_at_end=True,\n",
    "            fp16=True,\n",
    "            optim=\"adamw_torch\",\n",
    "            weight_decay=0.01\n",
    "        )\n",
    "        \n",
    "        # Initialize trainer\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            data_collator=DataCollatorWithPadding(self.tokenizer)\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        trainer.train()\n",
    "    \n",
    "    def predict(self, texts: List[str]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generate predictions with input validation\n",
    "        \"\"\"\n",
    "        # Validate input\n",
    "        self._validate_inputs(texts)\n",
    "        \n",
    "        # Prepare dataset\n",
    "        dataset = self.prepare_data(texts)\n",
    "        \n",
    "        # Create trainer for prediction\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer\n",
    "        )\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = trainer.predict(dataset)\n",
    "        return torch.nn.functional.softmax(\n",
    "            torch.tensor(predictions.predictions), \n",
    "            dim=1\n",
    "        )\n",
    "    \n",
    "    def semi_supervised_learning(\n",
    "        self,\n",
    "        labelled_texts: List[str],\n",
    "        labelled_labels: List[int],\n",
    "        unlabelled_texts: List[str],\n",
    "        confidence_threshold: float = 0.9\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Semi-supervised learning pipeline with comprehensive validation\n",
    "        \"\"\"\n",
    "        # Validate inputs\n",
    "        self._validate_inputs(labelled_texts, labelled_labels)\n",
    "        self._validate_inputs(unlabelled_texts)\n",
    "        \n",
    "        # Initial training\n",
    "        self.train(labelled_texts, labelled_labels)\n",
    "        \n",
    "        # Predict on unlabelled data\n",
    "        probabilities = self.predict(unlabelled_texts)\n",
    "        predictions = torch.argmax(probabilities, dim=1)\n",
    "        max_probs = torch.max(probabilities, dim=1)[0]\n",
    "        \n",
    "        # Filter high-confidence predictions\n",
    "        confident_idx = max_probs >= confidence_threshold\n",
    "        new_labels = predictions[confident_idx]\n",
    "        new_texts = [\n",
    "            unlabelled_texts[i] \n",
    "            for i in range(len(unlabelled_texts)) \n",
    "            if confident_idx[i]\n",
    "        ]\n",
    "        \n",
    "        # Combine datasets\n",
    "        all_texts = labelled_texts + new_texts\n",
    "        all_labels = labelled_labels + new_labels.tolist()\n",
    "        \n",
    "        # Retrain\n",
    "        self.train(all_texts, all_labels)\n",
    "        \n",
    "        # Final predictions\n",
    "        final_probabilities = self.predict(unlabelled_texts)\n",
    "        return torch.argmax(final_probabilities, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>Row Number</th>\n",
       "      <th>Tweet Treated</th>\n",
       "      <th>Label</th>\n",
       "      <th>Tweet Replaced</th>\n",
       "      <th>Raplaced</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fffeb77ff91c618cc5482e982240f1af9f09175cddf324...</td>\n",
       "      <td>19999</td>\n",
       "      <td>russia would save a ton of money if they'd pul...</td>\n",
       "      <td>Non Hate</td>\n",
       "      <td>russia would save a ton of money if they'd pul...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ffe7a960fccf628755ee70ab15e4fab5b45f0f436a2064...</td>\n",
       "      <td>19998</td>\n",
       "      <td>i hate grocery shopping. spent $112. damn you ...</td>\n",
       "      <td>Non Hate</td>\n",
       "      <td>i hate grocery shopping. spent $112. damn you ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ffdc308f7c1ceed12d8347ab9150551a9fe155023d624a...</td>\n",
       "      <td>19997</td>\n",
       "      <td>did you miss his blood and soil arguments f...</td>\n",
       "      <td>Non Hate</td>\n",
       "      <td>did you miss his blood and soil arguments f...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ffdbe9613a9ef0c6c484486e03422ab0bac73f62922005...</td>\n",
       "      <td>19996</td>\n",
       "      <td>on imperialism, too complex for twitter, but ...</td>\n",
       "      <td>Non Hate</td>\n",
       "      <td>on imperialism, too complex for twitter, but ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ffd75a28b8bf37f681e5d57d1dd1309df03aa3859a3d28...</td>\n",
       "      <td>19995</td>\n",
       "      <td>i'm still wondering why we don't #stop doing #...</td>\n",
       "      <td>Non Hate</td>\n",
       "      <td>i'm still wondering why we don't #stop doing #...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              row_id  Row Number  \\\n",
       "0  fffeb77ff91c618cc5482e982240f1af9f09175cddf324...       19999   \n",
       "1  ffe7a960fccf628755ee70ab15e4fab5b45f0f436a2064...       19998   \n",
       "2  ffdc308f7c1ceed12d8347ab9150551a9fe155023d624a...       19997   \n",
       "3  ffdbe9613a9ef0c6c484486e03422ab0bac73f62922005...       19996   \n",
       "4  ffd75a28b8bf37f681e5d57d1dd1309df03aa3859a3d28...       19995   \n",
       "\n",
       "                                       Tweet Treated     Label  \\\n",
       "0  russia would save a ton of money if they'd pul...  Non Hate   \n",
       "1  i hate grocery shopping. spent $112. damn you ...  Non Hate   \n",
       "2     did you miss his blood and soil arguments f...  Non Hate   \n",
       "3   on imperialism, too complex for twitter, but ...  Non Hate   \n",
       "4  i'm still wondering why we don't #stop doing #...  Non Hate   \n",
       "\n",
       "                                      Tweet Replaced  Raplaced  \n",
       "0  russia would save a ton of money if they'd pul...     False  \n",
       "1  i hate grocery shopping. spent $112. damn you ...     False  \n",
       "2     did you miss his blood and soil arguments f...     False  \n",
       "3   on imperialism, too complex for twitter, but ...      True  \n",
       "4  i'm still wondering why we don't #stop doing #...     False  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs_df = pd.read_csv(\"data/hs_dfUnamed.csv\")\n",
    "hs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>Row Number</th>\n",
       "      <th>Tweet Treated</th>\n",
       "      <th>Label</th>\n",
       "      <th>Tweet Replaced</th>\n",
       "      <th>Raplaced</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>fd03af0faea32d6ba18a883aa41d496a0f77a95f0ddfd3...</td>\n",
       "      <td>19768</td>\n",
       "      <td>zelensky and putin both need this lawnmower.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tatyana and Tatyana both need this lawnmower.</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>fc7449ecd89f35f4faa1549d7e99b49b89419f8a690f28...</td>\n",
       "      <td>19719</td>\n",
       "      <td>zelensky's talking trash. how's that for a p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mikhail's talking trash. how's that for a pr...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>fa08dfe829005b4fe02699c489a5273bc66afa827fb73e...</td>\n",
       "      <td>19536</td>\n",
       "      <td>yeah, zelensky had good intentions. but th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yeah, Mikhail had good intentions. but the...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>f353c03c58dcb208e9e952a00c216558fb32465708997d...</td>\n",
       "      <td>19000</td>\n",
       "      <td>again i am pretty sure china never called its...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>again i am pretty sure china never called its...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>f352d8b1a9d1e5ea36d768fb4a8342784c746c5d393e74...</td>\n",
       "      <td>18999</td>\n",
       "      <td>... which sickened &amp;gt;3000 un troops during t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>... which sickened &amp;gt;3000 un troops during t...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 row_id  Row Number  \\\n",
       "231   fd03af0faea32d6ba18a883aa41d496a0f77a95f0ddfd3...       19768   \n",
       "280   fc7449ecd89f35f4faa1549d7e99b49b89419f8a690f28...       19719   \n",
       "463   fa08dfe829005b4fe02699c489a5273bc66afa827fb73e...       19536   \n",
       "999   f353c03c58dcb208e9e952a00c216558fb32465708997d...       19000   \n",
       "1000  f352d8b1a9d1e5ea36d768fb4a8342784c746c5d393e74...       18999   \n",
       "\n",
       "                                          Tweet Treated Label  \\\n",
       "231       zelensky and putin both need this lawnmower.    NaN   \n",
       "280     zelensky's talking trash. how's that for a p...   NaN   \n",
       "463       yeah, zelensky had good intentions. but th...   NaN   \n",
       "999    again i am pretty sure china never called its...   NaN   \n",
       "1000  ... which sickened &gt;3000 un troops during t...   NaN   \n",
       "\n",
       "                                         Tweet Replaced  Raplaced  \n",
       "231      Tatyana and Tatyana both need this lawnmower.       True  \n",
       "280     Mikhail's talking trash. how's that for a pr...      True  \n",
       "463       yeah, Mikhail had good intentions. but the...      True  \n",
       "999    again i am pretty sure china never called its...      True  \n",
       "1000  ... which sickened &gt;3000 un troops during t...     False  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs_df_labelled = hs_df[hs_df[\"Label\"].notnull()]\n",
    "hs_df_unlabelled = hs_df[hs_df[\"Label\"].isnull()]\n",
    "\n",
    "hs_df_unlabelled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexandre\\AppData\\Local\\Temp\\ipykernel_24220\\3592870323.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  hs_df_labelled[\"Label_bool\"] = hs_df_labelled[\"Label\"].apply(lambda x: 1 if x == \"Hate\" else 0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>Row Number</th>\n",
       "      <th>Tweet Treated</th>\n",
       "      <th>Label</th>\n",
       "      <th>Tweet Replaced</th>\n",
       "      <th>Raplaced</th>\n",
       "      <th>Label_bool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fffeb77ff91c618cc5482e982240f1af9f09175cddf324...</td>\n",
       "      <td>19999</td>\n",
       "      <td>russia would save a ton of money if they'd pul...</td>\n",
       "      <td>Non Hate</td>\n",
       "      <td>russia would save a ton of money if they'd pul...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ffe7a960fccf628755ee70ab15e4fab5b45f0f436a2064...</td>\n",
       "      <td>19998</td>\n",
       "      <td>i hate grocery shopping. spent $112. damn you ...</td>\n",
       "      <td>Non Hate</td>\n",
       "      <td>i hate grocery shopping. spent $112. damn you ...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ffdc308f7c1ceed12d8347ab9150551a9fe155023d624a...</td>\n",
       "      <td>19997</td>\n",
       "      <td>did you miss his blood and soil arguments f...</td>\n",
       "      <td>Non Hate</td>\n",
       "      <td>did you miss his blood and soil arguments f...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ffdbe9613a9ef0c6c484486e03422ab0bac73f62922005...</td>\n",
       "      <td>19996</td>\n",
       "      <td>on imperialism, too complex for twitter, but ...</td>\n",
       "      <td>Non Hate</td>\n",
       "      <td>on imperialism, too complex for twitter, but ...</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ffd75a28b8bf37f681e5d57d1dd1309df03aa3859a3d28...</td>\n",
       "      <td>19995</td>\n",
       "      <td>i'm still wondering why we don't #stop doing #...</td>\n",
       "      <td>Non Hate</td>\n",
       "      <td>i'm still wondering why we don't #stop doing #...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>f361154d56c09c343ebc7d0f967c492eca88a0bbcd03fe...</td>\n",
       "      <td>19005</td>\n",
       "      <td>putin gave israeli pm bennett his word he woul...</td>\n",
       "      <td>Non Hate</td>\n",
       "      <td>Maksim gave israeli pm bennett his word he wou...</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>f35e48848e79e00d6bbbf10ca13c5e2287fcf07d6ecaf6...</td>\n",
       "      <td>19004</td>\n",
       "      <td>does anyone else ever feel like joy-crying whe...</td>\n",
       "      <td>Non Hate</td>\n",
       "      <td>does anyone else ever feel like joy-crying whe...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>f35abbe1259d13efb4031780f3226eb5ea20a385d225b2...</td>\n",
       "      <td>19003</td>\n",
       "      <td>nuremberg aint gonna actually happen until the...</td>\n",
       "      <td>Non Hate</td>\n",
       "      <td>nuremberg aint gonna actually happen until the...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>f35aace2208a8384202e020cfdb221889de2bcb87dea7d...</td>\n",
       "      <td>19002</td>\n",
       "      <td>their gov is also more corrupt than russi...</td>\n",
       "      <td>Non Hate</td>\n",
       "      <td>their gov is also more corrupt than russi...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>f3559624799a6fdb0e0748ef6bee9dc32101f4722456e5...</td>\n",
       "      <td>19001</td>\n",
       "      <td>it's his \"i'm a man of the people\" look. kind...</td>\n",
       "      <td>Non Hate</td>\n",
       "      <td>it's his \"i'm a man of the people\" look. kind...</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>996 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                row_id  Row Number  \\\n",
       "0    fffeb77ff91c618cc5482e982240f1af9f09175cddf324...       19999   \n",
       "1    ffe7a960fccf628755ee70ab15e4fab5b45f0f436a2064...       19998   \n",
       "2    ffdc308f7c1ceed12d8347ab9150551a9fe155023d624a...       19997   \n",
       "3    ffdbe9613a9ef0c6c484486e03422ab0bac73f62922005...       19996   \n",
       "4    ffd75a28b8bf37f681e5d57d1dd1309df03aa3859a3d28...       19995   \n",
       "..                                                 ...         ...   \n",
       "994  f361154d56c09c343ebc7d0f967c492eca88a0bbcd03fe...       19005   \n",
       "995  f35e48848e79e00d6bbbf10ca13c5e2287fcf07d6ecaf6...       19004   \n",
       "996  f35abbe1259d13efb4031780f3226eb5ea20a385d225b2...       19003   \n",
       "997  f35aace2208a8384202e020cfdb221889de2bcb87dea7d...       19002   \n",
       "998  f3559624799a6fdb0e0748ef6bee9dc32101f4722456e5...       19001   \n",
       "\n",
       "                                         Tweet Treated     Label  \\\n",
       "0    russia would save a ton of money if they'd pul...  Non Hate   \n",
       "1    i hate grocery shopping. spent $112. damn you ...  Non Hate   \n",
       "2       did you miss his blood and soil arguments f...  Non Hate   \n",
       "3     on imperialism, too complex for twitter, but ...  Non Hate   \n",
       "4    i'm still wondering why we don't #stop doing #...  Non Hate   \n",
       "..                                                 ...       ...   \n",
       "994  putin gave israeli pm bennett his word he woul...  Non Hate   \n",
       "995  does anyone else ever feel like joy-crying whe...  Non Hate   \n",
       "996  nuremberg aint gonna actually happen until the...  Non Hate   \n",
       "997       their gov is also more corrupt than russi...  Non Hate   \n",
       "998   it's his \"i'm a man of the people\" look. kind...  Non Hate   \n",
       "\n",
       "                                        Tweet Replaced  Raplaced  Label_bool  \n",
       "0    russia would save a ton of money if they'd pul...     False           0  \n",
       "1    i hate grocery shopping. spent $112. damn you ...     False           0  \n",
       "2       did you miss his blood and soil arguments f...     False           0  \n",
       "3     on imperialism, too complex for twitter, but ...      True           0  \n",
       "4    i'm still wondering why we don't #stop doing #...     False           0  \n",
       "..                                                 ...       ...         ...  \n",
       "994  Maksim gave israeli pm bennett his word he wou...      True           0  \n",
       "995  does anyone else ever feel like joy-crying whe...     False           0  \n",
       "996  nuremberg aint gonna actually happen until the...     False           0  \n",
       "997       their gov is also more corrupt than russi...     False           0  \n",
       "998   it's his \"i'm a man of the people\" look. kind...      True           0  \n",
       "\n",
       "[996 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs_df_labelled[\"Label_bool\"] = hs_df_labelled[\"Label\"].apply(lambda x: 1 if x == \"Hate\" else 0)\n",
    "hs_df_labelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_texts = hs_df_labelled[\"Tweet Replaced\"].tolist()\n",
    "labelled_labels = hs_df_labelled[\"Label_bool\"].tolist()\n",
    "\n",
    "unlabelled_texts = hs_df_unlabelled[\"Tweet Replaced\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPU Memory: 12.878086144 GB\n",
      "Initializing model with:\n",
      "Model: meta-llama/Llama-3.2-1B\n",
      "Number of Labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset with 996 texts and 996 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexandre\\miniconda3\\envs\\llama_ssl\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "  0%|          | 0/42 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "c:\\Users\\Alexandre\\miniconda3\\envs\\llama_ssl\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\Alexandre\\miniconda3\\envs\\llama_ssl\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "                                               \n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 14/42 [01:04<01:59,  4.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6099501848220825, 'eval_runtime': 2.1021, 'eval_samples_per_second': 47.095, 'eval_steps_per_second': 3.33, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexandre\\miniconda3\\envs\\llama_ssl\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\Alexandre\\miniconda3\\envs\\llama_ssl\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "                                               \n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 28/42 [02:07<01:00,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6078792810440063, 'eval_runtime': 2.1322, 'eval_samples_per_second': 46.431, 'eval_steps_per_second': 3.283, 'epoch': 1.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexandre\\miniconda3\\envs\\llama_ssl\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\Alexandre\\miniconda3\\envs\\llama_ssl\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42/42 [03:06<00:00,  4.30s/it]c:\\Users\\Alexandre\\miniconda3\\envs\\llama_ssl\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "                                               \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42/42 [03:09<00:00,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5143609642982483, 'eval_runtime': 2.1643, 'eval_samples_per_second': 45.743, 'eval_steps_per_second': 3.234, 'epoch': 2.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42/42 [03:09<00:00,  4.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 189.861, 'train_samples_per_second': 14.174, 'train_steps_per_second': 0.221, 'train_loss': 0.6058386393955776, 'epoch': 2.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexandre\\AppData\\Local\\Temp\\ipykernel_24220\\2170216504.py:236: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "c:\\Users\\Alexandre\\miniconda3\\envs\\llama_ssl\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2376/2376 [20:41<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset with 8625 texts and 8625 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexandre\\miniconda3\\envs\\llama_ssl\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "  0%|          | 0/363 [00:00<?, ?it/s]c:\\Users\\Alexandre\\miniconda3\\envs\\llama_ssl\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "                                                  \n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 121/363 [26:11<49:51, 12.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.140370488166809, 'eval_runtime': 45.8647, 'eval_samples_per_second': 18.794, 'eval_steps_per_second': 1.177, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexandre\\miniconda3\\envs\\llama_ssl\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\Alexandre\\miniconda3\\envs\\llama_ssl\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "                                                   \n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 243/363 [52:07<23:21, 11.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0746480226516724, 'eval_runtime': 45.876, 'eval_samples_per_second': 18.79, 'eval_steps_per_second': 1.177, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexandre\\miniconda3\\envs\\llama_ssl\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\Alexandre\\miniconda3\\envs\\llama_ssl\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 363/363 [1:17:01<00:00, 12.92s/it]c:\\Users\\Alexandre\\miniconda3\\envs\\llama_ssl\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "                                                   \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 363/363 [1:17:49<00:00, 12.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0968060493469238, 'eval_runtime': 47.7196, 'eval_samples_per_second': 18.064, 'eval_steps_per_second': 1.132, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 363/363 [1:17:50<00:00, 12.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 4670.0994, 'train_samples_per_second': 4.987, 'train_steps_per_second': 0.078, 'train_loss': 0.009961714100903388, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexandre\\AppData\\Local\\Temp\\ipykernel_24220\\2170216504.py:236: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "c:\\Users\\Alexandre\\miniconda3\\envs\\llama_ssl\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2376/2376 [19:43<00:00,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: tensor([0, 0, 0,  ..., 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize and run\n",
    "ssl_model = LlamaSemiSupervised(num_labels=2)\n",
    "predictions = ssl_model.semi_supervised_learning(\n",
    "    labelled_texts,\n",
    "    labelled_labels,\n",
    "    unlabelled_texts\n",
    ")\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 18053, 1: 951}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def count_predictions(predictions):\n",
    "    unique_elements, counts = torch.unique(predictions, return_counts=True)\n",
    "    return dict(zip(unique_elements.tolist(), counts.tolist()))\n",
    "\n",
    "# Example usage:\n",
    "#predictions = torch.tensor([1, 2, 2, 3, 3, 3, 4, 4, 4, 4])\n",
    "print(count_predictions(predictions))\n",
    "# Output: {1: 1, 2: 2, 3: 3, 4: 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet unlabeled</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tatyana and Tatyana both need this lawnmower.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mikhail's talking trash. how's that for a pr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yeah, Mikhail had good intentions. but the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>again i am pretty sure china never called its...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>... which sickened &amp;gt;3000 un troops during t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18999</th>\n",
       "      <td>let ukraine take care of itself.  this so-...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19000</th>\n",
       "      <td>your daily reminder that 141 un countries vo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19001</th>\n",
       "      <td>obvious to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19002</th>\n",
       "      <td>who is this f*cking idiot? biden isn't a neoco...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19003</th>\n",
       "      <td>Evgenia, you have stolen enough us tax dollar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19004 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         tweet unlabeled  pred\n",
       "0         Tatyana and Tatyana both need this lawnmower.      0\n",
       "1        Mikhail's talking trash. how's that for a pr...     0\n",
       "2          yeah, Mikhail had good intentions. but the...     0\n",
       "3       again i am pretty sure china never called its...     0\n",
       "4      ... which sickened &gt;3000 un troops during t...     0\n",
       "...                                                  ...   ...\n",
       "18999      let ukraine take care of itself.  this so-...     0\n",
       "19000    your daily reminder that 141 un countries vo...     0\n",
       "19001                                      obvious to...     0\n",
       "19002  who is this f*cking idiot? biden isn't a neoco...     0\n",
       "19003   Evgenia, you have stolen enough us tax dollar...     0\n",
       "\n",
       "[19004 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_list = predictions.tolist()\n",
    "\n",
    "unlabelled_pred = pd.DataFrame({'tweet unlabeled': unlabelled_texts, 'pred': predictions_list})\n",
    "unlabelled_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabelled_pred.loc[unlabelled_pred['pred'] == 1].to_csv('unlabelled_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_texts = hs_df_labelled[\"Tweet Treated\"].tolist()\n",
    "labelled_labels = hs_df_labelled[\"Label_bool\"].tolist()\n",
    "\n",
    "unlabelled_texts = hs_df_unlabelled[\"Tweet Treated\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 790, 1: 206})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_predictions(predictions):\n",
    "    return Counter(predictions)\n",
    "\n",
    "# Example usage:\n",
    "print(count_predictions(labelled_labels))\n",
    "# Output: Counter({4: 4, 3: 3, 2: 2, 1: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexandre\\AppData\\Local\\Temp\\ipykernel_24220\\2170216504.py:236: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2376/2376 [20:43<00:00,  1.91it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions_list = ssl_model.predict(unlabelled_texts)\n",
    "\n",
    "predictions_list = torch.argmax(predictions_list, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 790, 1: 206})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_predictions(predictions):\n",
    "    return Counter(predictions)\n",
    "\n",
    "# Example usage:\n",
    "print(count_predictions(labelled_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the model state dictionary\n",
    "torch.save(ssl_model.model.state_dict(), 'models/ssl_model_state_dict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPU Memory: 12.878086144 GB\n",
      "Initializing model with:\n",
      "Model: meta-llama/Llama-3.2-1B\n",
      "Number of Labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Alexandre\\AppData\\Local\\Temp\\ipykernel_24220\\677147971.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ssl_model_loaded.model.load_state_dict(torch.load('models/ssl_model_state_dict.pth', map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForSequenceClassification(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 2048, padding_idx=128001)\n",
       "        (layers): ModuleList(\n",
       "          (0-15): 16 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=2048, out_features=512, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=2048, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=2048, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=2048, out_features=2, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=2048, out_features=2, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new instance of LlamaSemiSupervised\n",
    "ssl_model_loaded = LlamaSemiSupervised(num_labels=2)\n",
    "\n",
    "# Load the model state dictionary\n",
    "ssl_model_loaded.model.load_state_dict(torch.load('models/ssl_model_state_dict.pth', map_location=device))\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "ssl_model_loaded.model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexandre\\AppData\\Local\\Temp\\ipykernel_24220\\2170216504.py:236: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "c:\\Users\\Alexandre\\miniconda3\\envs\\llama_ssl\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2376/2376 [19:54<00:00,  1.99it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions_list = ssl_model_loaded.predict(unlabelled_texts)\n",
    "\n",
    "predictions_list = torch.argmax(predictions_list, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 18303, 1: 701}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def count_predictions(predictions):\n",
    "    unique_elements, counts = torch.unique(predictions, return_counts=True)\n",
    "    return dict(zip(unique_elements.tolist(), counts.tolist()))\n",
    "\n",
    "# Example usage:\n",
    "#predictions = torch.tensor([1, 2, 2, 3, 3, 3, 4, 4, 4, 4])\n",
    "print(count_predictions(predictions_list))\n",
    "# Output: {1: 1, 2: 2, 3: 3, 4: 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssl_model.model.save_pretrained('models/ssl_model_test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_ssl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
